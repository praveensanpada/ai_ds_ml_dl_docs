ğŸ¯ Step 9: Model Deployment & Operations (MLOps) â€“ Advanced
âœ… 3) Model Monitoring & Maintenance (MLflow, TensorBoard) (IMP)

ğŸ§  What is Model Monitoring & Maintenance?
Once a model is deployed, it doesnâ€™t stop learning.
You must monitor, log, and update it continuously to ensure performance, reliability, and fairness in the real world.

ğŸ“˜ Real-Life Analogy
Training and deploying a model is like launching a rocket ğŸš€
But monitoring is the control room â€” checking telemetry, fixing issues, and keeping everything safe!

âœ… Why It's Important?
Reason	Why it Matters
ğŸ“‰ Model Drift	Data changes over time â†’ performance drops
ğŸ“Š Metrics Tracking	Track accuracy, loss, latency, etc.
ğŸ” Retraining Needed	Schedule retraining if quality decreases
ğŸ§ª Debugging	Detect bugs or input errors in real-time


âœ… Key Tools for Model Monitoring
Tool	Role
MLflow	Track experiments, model versions, metrics, parameters
TensorBoard	Visualize training logs (loss, accuracy, histograms)
Prometheus + Grafana	Monitor live metrics in production systems
Evidently	Track data drift, bias, and concept drift


âœ… What to Monitor?
What to Monitor	Example
ğŸ¯ Accuracy/Precision	Decreasing model quality
â± Inference Latency	Model responding slowly
ğŸ“Š Data Distribution	Input data drifting from training data
ğŸ§ª Error Rate	Increase in failed predictions
ğŸ’¬ User Feedback	Human-in-the-loop alerts



âœ… MLflow Example: Log Model & Metrics
pip install mlflow

import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load & train
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y)
model = RandomForestClassifier().fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)

# Log model & metrics
mlflow.start_run()
mlflow.log_metric("accuracy", acc)
mlflow.sklearn.log_model(model, "random_forest_model")
mlflow.end_run()


ğŸ–¥ Visit http://localhost:5000 to view MLflow dashboard (run mlflow ui in terminal)



âœ… TensorBoard Example: Monitor Deep Learning Training
from tensorflow.keras.callbacks import TensorBoard
import tensorflow as tf

# Create model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Setup TensorBoard callback
tb_callback = TensorBoard(log_dir='./logs', histogram_freq=1)

# Train model with logging
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[tb_callback])


Run this to view TensorBoard:
tensorboard --logdir=./logs




âœ… Summary Table
Tool	Purpose
MLflow	Logs parameters, metrics, models, versions
TensorBoard	Visualizes DL metrics and graphs
Prometheus/Grafana	Monitor infra/model in real-time
Evidently	Detects data drift and quality issues





âœ… Maintenance Practices
Task	Action
ğŸ”„ Retrain Periodically	Set up CRON jobs or CI/CD
ğŸ§ª Test New Models	Use A/B testing or shadow deployment
ğŸ“Š Alerting	Trigger alerts on metric thresholds
ğŸ§  Human Feedback Loop	Allow user corrections into retraining





