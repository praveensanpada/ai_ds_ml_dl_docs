ðŸŽ¯ Step 4: Advanced Machine Learning
âœ… Feature Engineering & Selection
ðŸ”¹ Includes:
Feature Creation
Feature Transformation
Feature Selection
Dimensionality Reduction (PCA, t-SNE, etc.)

ðŸš€ What is Feature Engineering?
Feature Engineering is the process of creating, modifying, or selecting features (input variables) to improve the performance of ML models.

ðŸ§  Why It's Important:
Removes irrelevant/noisy features
Reduces overfitting
Improves accuracy
Speeds up training

âœ… 1) Dimensionality Reduction (IMP)
Reduces the number of input variables while retaining most of the information.
Used when:
You have too many features
Your data is high-dimensional (images, text, genes)
You want to visualize complex datasets

=========================================================================================================

âœ… Method 1: PCA (Principal Component Analysis)
Linear method that projects data to a lower-dimensional space by maximizing variance.
ðŸ”¹ Real-Life Example:
Reduce pixel features of 64x64 image (4096 features) to 50 dimensions
Speeds up model and reduces noise

âœ… Python Example â€“ PCA
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Load dataset
data = load_iris()
X = data.data

# Apply PCA to reduce to 2 dimensions
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualize
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data.target, cmap='viridis')
plt.title("PCA: Iris Dataset")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

=========================================================================================================

âœ… Method 2: t-SNE (T-distributed Stochastic Neighbor Embedding)
Non-linear dimensionality reduction for visualizing high-dimensional data.
Captures local structure well
Best for visualization, not modeling
ðŸ”¹ Real-Life Example:
Visualize clusters of handwritten digits from MNIST (784-dim to 2D)

âœ… Python Example â€“ t-SNE
from sklearn.manifold import TSNE
from sklearn.datasets import load_iris

# Load data
X, y = load_iris(return_X_y=True)

# Apply t-SNE
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X)

# Plot
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='plasma')
plt.title("t-SNE: Iris Dataset")
plt.show()

=========================================================================================================

âœ… Other Dimensionality Reduction Methods
Method	Description	Use Case
UMAP	Fast, accurate, non-linear	Visualizing large complex datasets
LDA	Supervised, maximizes class separability	Classification tasks
Autoencoders	Deep learning for reduction	Images, text, compressed features
Kernel PCA	Non-linear PCA variant	Captures curves & complex patterns

=========================================================================================================

âœ… 2) Feature Selection Techniques
Select only the most relevant features from your dataset.

âœ… Filter Methods
Select features based on statistical tests
Tools: SelectKBest, chi2, f_classif

from sklearn.feature_selection import SelectKBest, f_classif

# Select top 2 features
selector = SelectKBest(score_func=f_classif, k=2)
X_new = selector.fit_transform(X, y)

=========================================================================================================

âœ… Wrapper Methods
Use model performance to evaluate combinations of features
Tools: RFE (Recursive Feature Elimination)
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
rfe = RFE(model, n_features_to_select=2)
X_rfe = rfe.fit_transform(X, y)

=========================================================================================================

âœ… Embedded Methods
Feature selection is part of model training (e.g., Lasso, Tree-based)
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X, y)

# Feature importance
importances = model.feature_importances_
print("Feature importances:", importances)

=========================================================================================================

âœ… Summary Table
Type	Technique	Best For
Feature Creation	Ratios, combinations	Derived insights (BMI, density)
Transformation	Scaling, encoding	ML-ready features (StandardScaler, OneHot)
Selection	RFE, SelectKBest	Irrelevant/noisy features
Dim Reduction	PCA, t-SNE, UMAP	Visualization, high-dimensional data

=========================================================================================================

âœ… Real-World Projects That Use These
Project	Feature Engineering Involved
Credit Risk Modeling	Select key features (income, loan, history)
Image Classification	PCA/Autoencoders for pixel reduction
Customer Segmentation	Create new ratio features, reduce dims (PCA)
NLP/Sentiment Analysis	Token count, TF-IDF, word embeddings

=========================================================================================================