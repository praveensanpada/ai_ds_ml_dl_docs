üéØ 2) Classification ‚Äì ML Concept (IMP)
Classification is about predicting a category or class label (e.g., spam/not spam, positive/negative).

‚úÖ Common Real-World Examples:
Problem	Output (Class)
Email spam detection	Spam / Not Spam
Disease diagnosis	Sick / Healthy
Credit card fraud detection	Fraud / Not Fraud
Image recognition	Dog / Cat / Car
Customer churn prediction	Churn / Not Churn

‚úÖ Popular Classification Algorithms (with Real Cases)

=====================================================================================================================

1Ô∏è‚É£ Logistic Regression
üìå Best for binary classification with linearly separable data.

üîπ Real-World Example:
Predict if a customer will buy a product based on age and income
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Use Iris dataset as binary example
X, y = load_iris(return_X_y=True)
X = X[y != 2]  # Binary (2 classes only)
y = y[y != 2]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Logistic model
model = LogisticRegression()
model.fit(X_train, y_train)
print("Logistic Regression Accuracy:", model.score(X_test, y_test))


=====================================================================================================================

2Ô∏è‚É£ Decision Tree Classifier
üìå Splits data using if/else rules ‚Äî easy to interpret.

üîπ Real-World Example:
Predict if a person has diabetes based on glucose level, BMI, etc.
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
model = DecisionTreeClassifier()
model.fit(X, y)
print("Decision Tree Prediction:", model.predict([X[0]]))

=====================================================================================================================

3Ô∏è‚É£ Random Forest Classifier
üìå Ensemble of decision trees = more accurate and robust

üîπ Real-World Example:
Loan approval prediction (based on salary, credit score, etc.)
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100)
model.fit(X, y)
print("Random Forest Accuracy:", model.score(X, y))

=====================================================================================================================

4Ô∏è‚É£ Support Vector Machine (SVM)
üìå Great for high-dimensional, clear margin separation

üîπ Real-World Example:
Cancer detection (Malignant vs Benign tumors)
from sklearn.svm import SVC

model = SVC(kernel='linear')
model.fit(X, y)
print("SVM Accuracy:", model.score(X, y))

=====================================================================================================================

5Ô∏è‚É£ K-Nearest Neighbors (KNN)
üìå Classifies based on majority class of nearest data points

üîπ Real-World Example:
Handwritten digit recognition (like MNIST)
from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier(n_neighbors=3)
model.fit(X, y)
print("KNN Accuracy:", model.score(X, y))

=====================================================================================================================

üß† Which Algorithm to Choose?
Algorithm	When to Use
Logistic	Simple, linear problems, quick baseline
Decision Tree	Easy to interpret, fast
Random Forest	High accuracy, handles missing values, works with many features
SVM	When you need margin-based classification, small datasets
KNN	Simple, good for small, clean datasets

=====================================================================================================================

üß™ Performance Metrics for Classification
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))

=====================================================================================================================

üìä Summary Table
Algorithm	Type	Best Use Case Example
Logistic	Binary	Purchase prediction
Decision Tree	Binary/Multiclass	Medical diagnosis
Random Forest	Binary/Multiclass	Loan approval, churn
SVM	Binary	Cancer detection (linear/non-linear)
KNN	Multiclass	Image classification, small datasets

=====================================================================================================================

‚úÖ Real Project Idea: Customer Churn Prediction
Goal: Predict if a telecom customer will leave
Features: Call duration, plan type, data usage
Target: Churn / Not Churn
Algorithms to Compare: Logistic, Tree, Random Forest, KNN, SVM
Metrics: Accuracy, F1 Score

=====================================================================================================================

