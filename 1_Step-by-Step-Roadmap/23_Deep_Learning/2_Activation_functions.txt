🎯 2) Activation Functions (IMP)
Used in Neural Networks to introduce non-linearity, helping the model learn complex patterns.

✅ What is an Activation Function?
An activation function determines whether a neuron should be activated (fire) or not, by applying a mathematical transformation to the input.

Without activation functions, neural networks would be just linear regression models — unable to learn non-linear relationships.

🔥 Common Activation Functions (with Visual & Code)

==========================================================================================================

✅ 1. ReLU (Rectified Linear Unit)
🧠 Formula:
🔹 Output:
0 if input < 0
x if input ≥ 0

✅ Pros:
Simple, fast
Solves vanishing gradient problem
Used in hidden layers

❌ Cons:
Can “die” (always output 0) if weights go negative permanently

📌 Real-Life Use:
All deep learning models like CNNs, RNNs, DNNs use ReLU by default

✅ Python Example:
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 100)
relu = np.maximum(0, x)

plt.plot(x, relu, label='ReLU')
plt.title("ReLU Activation")
plt.grid(True)
plt.legend()
plt.show()

==========================================================================================================

✅ 2. Sigmoid
🧠 Formula:

🔹 Output Range: (0, 1)
✅ Pros:
Good for binary classification (logistic regression)

Output can be interpreted as probability

❌ Cons:
Vanishing gradient

Output saturates at 0 or 1

📌 Real-Life Use:
Final layer of binary classifier

✅ Python Example:
sigmoid = 1 / (1 + np.exp(-x))
plt.plot(x, sigmoid, label='Sigmoid')
plt.title("Sigmoid Activation")
plt.grid(True)
plt.legend()
plt.show()

==========================================================================================================

✅ 3. Tanh (Hyperbolic Tangent)
🧠 Formula:

🔹 Output Range: (-1, 1)
✅ Pros:
Zero-centered (unlike sigmoid)

Stronger gradients than sigmoid

❌ Cons:
Still suffers from vanishing gradients for large values

📌 Real-Life Use:
In older RNNs or simple hidden layers

✅ Python Example:
tanh = np.tanh(x)
plt.plot(x, tanh, label='Tanh')
plt.title("Tanh Activation")
plt.grid(True)
plt.legend()
plt.show()

==========================================================================================================

✅ 4. Softmax
🧠 Formula:
🔹 Output:
Converts raw output into probabilities

Sum of outputs = 1

✅ Pros:
Great for multi-class classification

Each output neuron represents a class

❌ Cons:
Not used in hidden layers

📌 Real-Life Use:
Final layer of models predicting multiple classes (e.g., digit classification)

✅ Python Example:
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

x_vals = np.array([1.0, 2.0, 3.0])
print("Softmax:", softmax(x_vals))  # Output: Probabilities summing to 1

==========================================================================================================

✅ Summary Table: Activation Functions
Function	Output Range	Used In	Pros	Cons
ReLU	0 to ∞	Hidden layers in all DL models	Fast, simple, avoids vanishing	Dying neurons
Sigmoid	0 to 1	Binary output layer	Probabilities	Vanishing gradients
Tanh	-1 to 1	RNNs, hidden layers	Zero-centered	Vanishing gradients
Softmax	0 to 1	Multi-class output layer	Probability distribution	Computationally expensive

✅ Real-World Example by Use Case:
Task	Output Layer Activation
Spam detection (0/1)	Sigmoid
Image classification (0-9)	Softmax
Regression (predict number)	None or ReLU
Hidden layers (general)	ReLU or Tanh

