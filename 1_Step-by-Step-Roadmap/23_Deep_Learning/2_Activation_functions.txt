ðŸŽ¯ 2) Activation Functions (IMP)
Used in Neural Networks to introduce non-linearity, helping the model learn complex patterns.

âœ… What is an Activation Function?
An activation function determines whether a neuron should be activated (fire) or not, by applying a mathematical transformation to the input.

Without activation functions, neural networks would be just linear regression models â€” unable to learn non-linear relationships.

ðŸ”¥ Common Activation Functions (with Visual & Code)

==========================================================================================================

âœ… 1. ReLU (Rectified Linear Unit)
ðŸ§  Formula:
ðŸ”¹ Output:
0 if input < 0
x if input â‰¥ 0

âœ… Pros:
Simple, fast
Solves vanishing gradient problem
Used in hidden layers

âŒ Cons:
Can â€œdieâ€ (always output 0) if weights go negative permanently

ðŸ“Œ Real-Life Use:
All deep learning models like CNNs, RNNs, DNNs use ReLU by default

âœ… Python Example:
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-10, 10, 100)
relu = np.maximum(0, x)

plt.plot(x, relu, label='ReLU')
plt.title("ReLU Activation")
plt.grid(True)
plt.legend()
plt.show()

==========================================================================================================

âœ… 2. Sigmoid
ðŸ§  Formula:

ðŸ”¹ Output Range: (0, 1)
âœ… Pros:
Good for binary classification (logistic regression)

Output can be interpreted as probability

âŒ Cons:
Vanishing gradient

Output saturates at 0 or 1

ðŸ“Œ Real-Life Use:
Final layer of binary classifier

âœ… Python Example:
sigmoid = 1 / (1 + np.exp(-x))
plt.plot(x, sigmoid, label='Sigmoid')
plt.title("Sigmoid Activation")
plt.grid(True)
plt.legend()
plt.show()

==========================================================================================================

âœ… 3. Tanh (Hyperbolic Tangent)
ðŸ§  Formula:

ðŸ”¹ Output Range: (-1, 1)
âœ… Pros:
Zero-centered (unlike sigmoid)

Stronger gradients than sigmoid

âŒ Cons:
Still suffers from vanishing gradients for large values

ðŸ“Œ Real-Life Use:
In older RNNs or simple hidden layers

âœ… Python Example:
tanh = np.tanh(x)
plt.plot(x, tanh, label='Tanh')
plt.title("Tanh Activation")
plt.grid(True)
plt.legend()
plt.show()

==========================================================================================================

âœ… 4. Softmax
ðŸ§  Formula:
ðŸ”¹ Output:
Converts raw output into probabilities

Sum of outputs = 1

âœ… Pros:
Great for multi-class classification

Each output neuron represents a class

âŒ Cons:
Not used in hidden layers

ðŸ“Œ Real-Life Use:
Final layer of models predicting multiple classes (e.g., digit classification)

âœ… Python Example:
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

x_vals = np.array([1.0, 2.0, 3.0])
print("Softmax:", softmax(x_vals))  # Output: Probabilities summing to 1

==========================================================================================================

âœ… Summary Table: Activation Functions
Function	Output Range	Used In	Pros	Cons
ReLU	0 to âˆž	Hidden layers in all DL models	Fast, simple, avoids vanishing	Dying neurons
Sigmoid	0 to 1	Binary output layer	Probabilities	Vanishing gradients
Tanh	-1 to 1	RNNs, hidden layers	Zero-centered	Vanishing gradients
Softmax	0 to 1	Multi-class output layer	Probability distribution	Computationally expensive

âœ… Real-World Example by Use Case:
Task	Output Layer Activation
Spam detection (0/1)	Sigmoid
Image classification (0-9)	Softmax
Regression (predict number)	None or ReLU
Hidden layers (general)	ReLU or Tanh

