ðŸŽ¯ Classification Evaluation Metrics (IMP)
These metrics help evaluate how well a model performs on categorical outputs, such as:
Spam or Not Spam
Fraud or Not Fraud
Disease or No Disease

====================================================================================================================

âœ… Real-Life Example: Spam Detection
Letâ€™s say your model classifies emails as:

Type	Count
Spam	100
Not Spam	900
Total	1000

====================================================================================================================

Now, letâ€™s assume the model makes these predictions:

Prediction â†’	Spam	Not Spam
Actual Spam	80	20
Actual Not	30	870

====================================================================================================================

âœ… Confusion Matrix
Predicted: Spam	Predicted: Not Spam
Actual: Spam	TP = 80	FN = 20
Actual: Not Spam	FP = 30	TN = 870

====================================================================================================================

ðŸ”¹ Terms You Must Know:
Term	Meaning
TP	True Positives (correctly predicted Spam)
FP	False Positives (wrongly predicted Spam)
TN	True Negatives (correctly predicted Not Spam)
FN	False Negatives (missed actual Spam)

====================================================================================================================

âœ… 1. Accuracy
ðŸ“Œ How often is the model correct?

accuracy = (80 + 870) / (80 + 870 + 30 + 20) = 0.95 (95%)

âœ… Good when classes are balanced
ðŸš« Misleading if classes are imbalanced

====================================================================================================================

âœ… 2. Precision
ðŸ“Œ Of all predicted Spam, how many were actually Spam?

precision = 80 / (80 + 30) = 0.727 (72.7%)

âœ… High precision = fewer false alarms

====================================================================================================================

âœ… 3. Recall (Sensitivity / TPR)
ðŸ“Œ Of all actual Spam, how many did we catch?

recall = 80 / (80 + 20) = 0.80 (80%)

âœ… High recall = fewer missed actual spam emails

====================================================================================================================

âœ… 4. F1-Score
ðŸ“Œ Harmonic mean of Precision and Recall
Balances both

f1 = 2 * (0.727 * 0.80) / (0.727 + 0.80) â‰ˆ 0.76

âœ… Use when you need balance between precision & recall

====================================================================================================================

âœ… 5. ROC-AUC Score (Receiver Operating Characteristic)
ðŸ“Œ Measures how well the model separates classes across thresholds.
ROC Curve: plots TPR vs FPR
AUC = Area Under Curve, ranges from 0 to 1
Closer to 1 = better

====================================================================================================================

âœ… 5. ROC-AUC Score (Receiver Operating Characteristic)
ðŸ“Œ Measures how well the model separates classes across thresholds.
ROC Curve: plots TPR vs FPR
AUC = Area Under Curve, ranges from 0 to 1
Closer to 1 = better

from sklearn.metrics import roc_auc_score

# y_true = [0, 1, 1, 0]  (actual)
# y_prob = [0.1, 0.9, 0.8, 0.3]  (predicted probability)
roc_auc_score(y_true, y_prob)

âœ… Best for evaluating probabilistic models, especially imbalanced classes

====================================================================================================================

âœ… Python Code Example (All Metrics)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer

# Load real binary classification dataset
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Metrics
print("âœ… Accuracy:", accuracy_score(y_test, y_pred))
print("âœ… Precision:", precision_score(y_test, y_pred))
print("âœ… Recall:", recall_score(y_test, y_pred))
print("âœ… F1 Score:", f1_score(y_test, y_pred))
print("âœ… ROC-AUC:", roc_auc_score(y_test, y_prob))

====================================================================================================================

âœ… Summary Table
Metric	Best When...	Range
Accuracy	Balanced classes	0 to 1
Precision	False positives are costly	0 to 1
Recall	Missing positives is costly	0 to 1
F1 Score	You need a balance	0 to 1
ROC-AUC	Need class-separation measure	0 to 1

====================================================================================================================

âœ… Real-World Use Cases
Problem	Focus On
Email Spam Detection	Precision (avoid false positives)
Cancer Diagnosis	Recall (catch all actual cases)
Fraud Detection	ROC-AUC + F1 (imbalanced)
Churn Prediction	F1 + ROC

====================================================================================================================
