🎯 Advanced DL Concepts
✅ 2) Autoencoders (IMP)
(Autoencoders are unsupervised neural networks used for data compression, denoising, and feature learning.)

🧠 What is an Autoencoder?
An Autoencoder is a type of neural network that learns to compress (encode) data into a lower dimension, and then reconstruct (decode) it back to the original.

It’s trained to reproduce its own input.

🧱 Autoencoder Structure:
Input → Encoder → Bottleneck (compressed) → Decoder → Output (reconstructed input)
Encoder: Learns to compress the data
Decoder: Learns to reconstruct it
Bottleneck: Core representation (latent space)

✅ Real-Life Use Cases of Autoencoders
Domain	Use Case
📦 Compression	Reduce image size while retaining quality
🧹 Denoising	Remove noise from images/audio
🔐 Anomaly Detection	Learn normal → detect what's different
🎨 Feature Learning	Extract abstract features for clustering
🧠 Pretraining	Use as unsupervised layer in deep nets

✅ Python Code: Autoencoder on MNIST (Digit Images)
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt

# Load and normalize MNIST
(X_train, _), (X_test, _) = mnist.load_data()
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0
X_train = X_train.reshape(-1, 784)
X_test = X_test.reshape(-1, 784)

# Autoencoder architecture
input_dim = X_train.shape[1]
encoding_dim = 64  # compressed layer

# Encoder
input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_dim, activation='relu')(input_layer)

# Decoder
decoded = Dense(input_dim, activation='sigmoid')(encoded)

# Autoencoder model
autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Train
autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_data=(X_test, X_test))

# Predict and visualize
decoded_imgs = autoencoder.predict(X_test)

# Show original and reconstructed
n = 5
plt.figure(figsize=(10, 2))
for i in range(n):
    # Original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')
    ax.axis('off')

    # Reconstructed
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')
    ax.axis('off')
plt.show()

✅ Key Terms
Term	Description
Encoder	Compresses input into a smaller representation
Latent space	The compressed, encoded version of the input
Decoder	Reconstructs the original input from latent space
Reconstruction Loss	Measures how different the output is from input (e.g., MSE)

✅ Autoencoder Variants
Type	Use Case
Vanilla Autoencoder	Basic compression/reconstruction
Denoising Autoencoder	Learns to remove noise from input
Sparse Autoencoder	Encourages sparse (few active) neurons
Variational Autoencoder (VAE)	Probabilistic, for generative tasks
Convolutional Autoencoder	Works with images using Conv2D

✅ Real-World Applications
Industry	Use Case
Healthcare	Anomaly detection in ECG/X-rays
Manufacturing	Fault detection in sensor data
Retail	Compress user features for recommendation
Finance	Detect fraud by comparing reconstruction error

✅ When to Use Autoencoders
Situation	Use Autoencoders?
You want to reduce feature size	✅ Yes
You want to remove noise	✅ Yes
You want to generate new data	✅ Use VAE
You have labeled data	❌ Use supervised models instead

✅ Summary
Component	Role
🔧 Encoder	Compresses high-dim input to low-dim latent space
🧠 Bottleneck	Core representation of input
🔄 Decoder	Reconstructs input from latent space
📉 Loss	How close output is to the original (MSE, BCE)


