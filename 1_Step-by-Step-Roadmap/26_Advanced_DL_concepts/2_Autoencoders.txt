ğŸ¯ Advanced DL Concepts
âœ… 2) Autoencoders (IMP)
(Autoencoders are unsupervised neural networks used for data compression, denoising, and feature learning.)

ğŸ§  What is an Autoencoder?
An Autoencoder is a type of neural network that learns to compress (encode) data into a lower dimension, and then reconstruct (decode) it back to the original.

Itâ€™s trained to reproduce its own input.

ğŸ§± Autoencoder Structure:
Input â†’ Encoder â†’ Bottleneck (compressed) â†’ Decoder â†’ Output (reconstructed input)
Encoder: Learns to compress the data
Decoder: Learns to reconstruct it
Bottleneck: Core representation (latent space)

âœ… Real-Life Use Cases of Autoencoders
Domain	Use Case
ğŸ“¦ Compression	Reduce image size while retaining quality
ğŸ§¹ Denoising	Remove noise from images/audio
ğŸ” Anomaly Detection	Learn normal â†’ detect what's different
ğŸ¨ Feature Learning	Extract abstract features for clustering
ğŸ§  Pretraining	Use as unsupervised layer in deep nets

âœ… Python Code: Autoencoder on MNIST (Digit Images)
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt

# Load and normalize MNIST
(X_train, _), (X_test, _) = mnist.load_data()
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0
X_train = X_train.reshape(-1, 784)
X_test = X_test.reshape(-1, 784)

# Autoencoder architecture
input_dim = X_train.shape[1]
encoding_dim = 64  # compressed layer

# Encoder
input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_dim, activation='relu')(input_layer)

# Decoder
decoded = Dense(input_dim, activation='sigmoid')(encoded)

# Autoencoder model
autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Train
autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_data=(X_test, X_test))

# Predict and visualize
decoded_imgs = autoencoder.predict(X_test)

# Show original and reconstructed
n = 5
plt.figure(figsize=(10, 2))
for i in range(n):
    # Original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')
    ax.axis('off')

    # Reconstructed
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')
    ax.axis('off')
plt.show()

âœ… Key Terms
Term	Description
Encoder	Compresses input into a smaller representation
Latent space	The compressed, encoded version of the input
Decoder	Reconstructs the original input from latent space
Reconstruction Loss	Measures how different the output is from input (e.g., MSE)

âœ… Autoencoder Variants
Type	Use Case
Vanilla Autoencoder	Basic compression/reconstruction
Denoising Autoencoder	Learns to remove noise from input
Sparse Autoencoder	Encourages sparse (few active) neurons
Variational Autoencoder (VAE)	Probabilistic, for generative tasks
Convolutional Autoencoder	Works with images using Conv2D

âœ… Real-World Applications
Industry	Use Case
Healthcare	Anomaly detection in ECG/X-rays
Manufacturing	Fault detection in sensor data
Retail	Compress user features for recommendation
Finance	Detect fraud by comparing reconstruction error

âœ… When to Use Autoencoders
Situation	Use Autoencoders?
You want to reduce feature size	âœ… Yes
You want to remove noise	âœ… Yes
You want to generate new data	âœ… Use VAE
You have labeled data	âŒ Use supervised models instead

âœ… Summary
Component	Role
ğŸ”§ Encoder	Compresses high-dim input to low-dim latent space
ğŸ§  Bottleneck	Core representation of input
ğŸ”„ Decoder	Reconstructs input from latent space
ğŸ“‰ Loss	How close output is to the original (MSE, BCE)


